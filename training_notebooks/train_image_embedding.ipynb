{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2OcdFhCwdpK"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBuZ5VK_N0GS"
      },
      "source": [
        "## Imports, gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2H4h3Gevp7H",
        "outputId": "32bb5a68-379c-4235-e566-659c03f08c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.gen_math_ops import truncate_mod\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from ResNet import *\n",
        "from InceptionNet import *\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model \n",
        "\n",
        "\n",
        "from pathlib import Path \n",
        "!pip install -q -U tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path \n",
        "mnt = Path('/content/drive')\n",
        "drive.mount(str(mnt))\n",
        "\n",
        "mydrive = Path(mnt) / 'MyDrive' / 'Colab Notebooks' / 'DL for Optical Imaging' / 'Project'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2MXuV9N51hl",
        "outputId": "bfabb9ee-dd37-452a-f499-1f6b1fd6bc73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/Colab Notebooks/DL for Optical Imaging/Project')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLKZJOom_Pw6",
        "outputId": "26220672-3d3c-40f2-c146-f913f802c9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 29 16:39:48 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X2wB1P_woqz"
      },
      "outputs": [],
      "source": [
        "def load(filename, key=None):\n",
        "    # use a context manager\n",
        "    # to avoid leaks\n",
        "    with np.load(filename) as data:\n",
        "        if not key:\n",
        "            print(f'The keys are {list(data.keys())}.')\n",
        "            return\n",
        "        else:\n",
        "            return data[key]\n",
        "\n",
        "NClass        = 49\n",
        "\n",
        "\n",
        "filename = mydrive / 'audVisIdn.npz'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXe1bvDMwckD"
      },
      "source": [
        "## Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ZjksHbvydo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_addons as tfa\n",
        "from pathlib import Path\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "SMALL = 0\n",
        "\n",
        "\n",
        "class dataHolder():\n",
        "  def __init__(self, directory=Path(mydrive), to_cat=False):\n",
        "    self.prepareData(directory)\n",
        "                      \n",
        "  def prepareData(self, directory):\n",
        "    ADD = '_64' if SMALL else '_160'\n",
        "    self.trainlabels = np.load(directory / f'train_lab_faces_only{ADD}.npy')\n",
        "    self.vallabels = np.load(directory / f'val_lab_faces_only{ADD}.npy')\n",
        "    self.testlabels =  np.load(directory / f'test_lab_faces_only{ADD}.npy')\n",
        "    # ~ 5 gigs RAM overall\n",
        "    self.trainimg = np.load(directory / f'train_faces_only{ADD}.npy')\n",
        "    self.valimg = np.load(directory / f'val_faces_only{ADD}.npy')\n",
        "    self.testimg =  np.load(directory / f'test_faces_only{ADD}.npy')\n",
        "\n",
        "  def getImagesFromPerson(self, label, dataset='train'):\n",
        "    if dataset=='train':\n",
        "        img = self.trainimg; lab = self.trainlabels\n",
        "    elif dataset=='val':\n",
        "        img = self.valimg; lab = self.vallabels\n",
        "    elif dataset=='test':\n",
        "        img = self.testimg; lab = self.testlabels\n",
        "    relevant = np.where(lab == label)[0]\n",
        "    imgs = [img[i,:,:,:] for i in relevant]\n",
        "    return imgs\n",
        "          \n",
        "  def seeImageFromPerson(self, label, dataset='train', limit=5):\n",
        "    imgs = self.getImagesFromPerson(label, dataset)\n",
        "    count = 0\n",
        "    for i  in range(len(imgs)):\n",
        "        plt.figure(); plt.imshow(imgs[i])\n",
        "        plt.waitforbuttonpress()\n",
        "        count += 1\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "data = dataHolder()\n",
        "\n",
        "train_out = tf.keras.utils.to_categorical(data.trainlabels, 49)\n",
        "val_out = tf.keras.utils.to_categorical(data.vallabels, 49)\n",
        "test_out = tf.keras.utils.to_categorical(data.testlabels, 49)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvTn4U1rL2NC"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuRg5R5oMDK5"
      },
      "source": [
        "## Inception ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzNN-Kg70Rur"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "from keras import backend\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.engine import training\n",
        "from keras.layers import VersionAwareLayers\n",
        "from keras.utils import data_utils\n",
        "from keras.utils import layer_utils\n",
        "\n",
        "\n",
        "BASE_WEIGHT_URL = ('https://storage.googleapis.com/tensorflow/'\n",
        "                   'keras-applications/inception_resnet_v2/')\n",
        "layers = None\n",
        "\n",
        "\n",
        "def InceptionResNetV2(include_top=True,\n",
        "                      weights='imagenet',\n",
        "                      input_tensor=None,\n",
        "                      input_shape=None,\n",
        "                      pooling=None,\n",
        "                      classes=1000,\n",
        "                      classifier_activation='softmax',\n",
        "                      **kwargs):\n",
        "  global layers\n",
        "  if 'layers' in kwargs:\n",
        "    layers = kwargs.pop('layers')\n",
        "  else:\n",
        "    layers = VersionAwareLayers()\n",
        "  if kwargs:\n",
        "    raise ValueError('Unknown argument(s): %s' % (kwargs,))\n",
        "  if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n",
        "    raise ValueError('The `weights` argument should be either '\n",
        "                     '`None` (random initialization), `imagenet` '\n",
        "                     '(pre-training on ImageNet), '\n",
        "                     'or the path to the weights file to be loaded.')\n",
        "\n",
        "  if weights == 'imagenet' and include_top and classes != 1000:\n",
        "    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
        "                     ' as true, `classes` should be 1000')\n",
        "\n",
        "  # Determine proper input shape\n",
        "  input_shape = imagenet_utils.obtain_input_shape(\n",
        "      input_shape,\n",
        "      default_size=299,\n",
        "      min_size=75,\n",
        "      data_format=backend.image_data_format(),\n",
        "      require_flatten=include_top,\n",
        "      weights=weights)\n",
        "\n",
        "  if input_tensor is None:\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "  else:\n",
        "    if not backend.is_keras_tensor(input_tensor):\n",
        "      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "    else:\n",
        "      img_input = input_tensor\n",
        "\n",
        "  # Stem block: 35 x 35 x 192\n",
        "  x = conv2d_bn(img_input, 32, 3, strides=2, padding='valid')\n",
        "  x = conv2d_bn(x, 32, 3, padding='valid')\n",
        "  # x = conv2d_bn(x, 64, 3)\n",
        "  x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "  x = conv2d_bn(x, 80, 1, padding='valid')\n",
        "  x = conv2d_bn(x, 192, 3, padding='valid')\n",
        "  x = layers.MaxPooling2D(3, strides=2)(x)\n",
        "\n",
        "  # Mixed 5b (Inception-A block): 35 x 35 x 320\n",
        "  branch_0 = conv2d_bn(x, 96, 1)\n",
        "  branch_1 = conv2d_bn(x, 48, 1)\n",
        "  branch_1 = conv2d_bn(branch_1, 64, 5)\n",
        "  branch_2 = conv2d_bn(x, 64, 1)\n",
        "  # branch_2 = conv2d_bn(branch_2, 96, 3)\n",
        "  branch_2 = conv2d_bn(branch_2, 96, 3)\n",
        "  branch_pool = layers.AveragePooling2D(3, strides=1, padding='same')(x)\n",
        "  branch_pool = conv2d_bn(branch_pool, 64, 1)\n",
        "  branches = [branch_0, branch_1, branch_2, branch_pool]\n",
        "  channel_axis = 1 if backend.image_data_format() == 'channels_first' else 3\n",
        "  x = layers.Concatenate(axis=channel_axis, name='mixed_5b')(branches)\n",
        "\n",
        "  # 2x block35 (Inception-ResNet-A block): 35 x 35 x 320\n",
        "  for block_idx in range(1, 3):\n",
        "    x = inception_resnet_block(\n",
        "        x, scale=0.17, block_type='block35', block_idx=block_idx)\n",
        "\n",
        "  # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n",
        "  branch_0 = conv2d_bn(x, 384, 3, strides=2, padding='valid')\n",
        "  branch_1 = conv2d_bn(x, 256, 1)\n",
        "  # branch_1 = conv2d_bn(branch_1, 256, 3)\n",
        "  branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding='valid')\n",
        "  branch_pool = layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
        "  branches = [branch_0, branch_1, branch_pool]\n",
        "  x = layers.Concatenate(axis=channel_axis, name='mixed_6a')(branches)\n",
        "\n",
        "  # 2x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n",
        "  for block_idx in range(1, 3):\n",
        "    x = inception_resnet_block(\n",
        "        x, scale=0.1, block_type='block17', block_idx=block_idx)\n",
        "\n",
        "  # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n",
        "  branch_0 = conv2d_bn(x, 256, 1)\n",
        "  branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding='valid')\n",
        "  branch_1 = conv2d_bn(x, 256, 1)\n",
        "  branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding='valid')\n",
        "  branch_2 = conv2d_bn(x, 256, 1)\n",
        "  # branch_2 = conv2d_bn(branch_2, 288, 3)\n",
        "  branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding='valid')\n",
        "  branch_pool = layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
        "  branches = [branch_0, branch_1, branch_2, branch_pool]\n",
        "  x = layers.Concatenate(axis=channel_axis, name='mixed_7a')(branches)\n",
        "\n",
        "  # 2x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n",
        "  for block_idx in range(1, 3):\n",
        "    x = inception_resnet_block(\n",
        "        x, scale=0.2, block_type='block8', block_idx=block_idx)\n",
        "  x = inception_resnet_block(\n",
        "      x, scale=1., activation=None, block_type='block8', block_idx=10)\n",
        "\n",
        "  # Final convolution block: 8 x 8 x 1536\n",
        "  x = conv2d_bn(x, 1536, 1, name='conv_7b')\n",
        "\n",
        "  if include_top:\n",
        "    # Classification block\n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    imagenet_utils.validate_activation(classifier_activation, weights)\n",
        "    x = layers.Dense(classes, activation=classifier_activation,\n",
        "                     name='predictions')(x)\n",
        "  else:\n",
        "    if pooling == 'avg':\n",
        "      x = layers.GlobalAveragePooling2D()(x)\n",
        "    elif pooling == 'max':\n",
        "      x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "  # Ensure that the model takes into account\n",
        "  # any potential predecessors of `input_tensor`.\n",
        "  if input_tensor is not None:\n",
        "    inputs = layer_utils.get_source_inputs(input_tensor)\n",
        "  else:\n",
        "    inputs = img_input\n",
        "\n",
        "  # Create model.\n",
        "  model = training.Model(inputs, x, name='inception_resnet_v2')\n",
        "\n",
        "  # Load weights.\n",
        "  if weights == 'imagenet':\n",
        "    if include_top:\n",
        "      fname = 'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "      weights_path = data_utils.get_file(\n",
        "          fname,\n",
        "          BASE_WEIGHT_URL + fname,\n",
        "          cache_subdir='models',\n",
        "          file_hash='e693bd0210a403b3192acc6073ad2e96')\n",
        "    else:\n",
        "      fname = ('inception_resnet_v2_weights_'\n",
        "               'tf_dim_ordering_tf_kernels_notop.h5')\n",
        "      weights_path = data_utils.get_file(\n",
        "          fname,\n",
        "          BASE_WEIGHT_URL + fname,\n",
        "          cache_subdir='models',\n",
        "          file_hash='d19885ff4a710c122648d3b5c3b684e4')\n",
        "    model.load_weights(weights_path)\n",
        "  elif weights is not None:\n",
        "    model.load_weights(weights)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def conv2d_bn(x,\n",
        "              filters,\n",
        "              kernel_size,\n",
        "              strides=1,\n",
        "              padding='same',\n",
        "              activation='relu',\n",
        "              use_bias=False,\n",
        "              name=None):\n",
        "\n",
        "  x = layers.SeparableConv2D(\n",
        "      filters,\n",
        "      kernel_size,\n",
        "      strides=strides,\n",
        "      padding=padding,\n",
        "      use_bias=use_bias,\n",
        "      name=name)(\n",
        "          x)\n",
        "  if not use_bias:\n",
        "    bn_axis = 1 if backend.image_data_format() == 'channels_first' else 3\n",
        "    bn_name = None if name is None else name + '_bn'\n",
        "    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
        "  if activation is not None:\n",
        "    ac_name = None if name is None else name + '_ac'\n",
        "    x = layers.Activation(activation, name=ac_name)(x)\n",
        "  return x\n",
        "\n",
        "\n",
        "def inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n",
        "\n",
        "  if block_type == 'block35':\n",
        "    branch_0 = conv2d_bn(x, 32, 1)\n",
        "    branch_1 = conv2d_bn(x, 32, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 32, 3)\n",
        "    branch_2 = conv2d_bn(x, 32, 1)\n",
        "    # branch_2 = conv2d_bn(branch_2, 48, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 64, 3)\n",
        "    branches = [branch_0, branch_1, branch_2]\n",
        "  elif block_type == 'block17':\n",
        "    branch_0 = conv2d_bn(x, 192, 1)\n",
        "    branch_1 = conv2d_bn(x, 128, 1)\n",
        "    # branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n",
        "    branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n",
        "    branches = [branch_0, branch_1]\n",
        "  elif block_type == 'block8':\n",
        "    branch_0 = conv2d_bn(x, 192, 1)\n",
        "    branch_1 = conv2d_bn(x, 192, 1)\n",
        "    # branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n",
        "    branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n",
        "    branches = [branch_0, branch_1]\n",
        "  else:\n",
        "    raise ValueError('Unknown Inception-ResNet block type. '\n",
        "                     'Expects \"block35\", \"block17\" or \"block8\", '\n",
        "                     'but got: ' + str(block_type))\n",
        "\n",
        "  block_name = block_type + '_' + str(block_idx)\n",
        "  channel_axis = 1 if backend.image_data_format() == 'channels_first' else 3\n",
        "  mixed = layers.Concatenate(\n",
        "      axis=channel_axis, name=block_name + '_mixed')(\n",
        "          branches)\n",
        "  up = conv2d_bn(\n",
        "      mixed,\n",
        "      backend.int_shape(x)[channel_axis],\n",
        "      1,\n",
        "      activation=None,\n",
        "      use_bias=True,\n",
        "      name=block_name + '_conv')\n",
        "\n",
        "  x = layers.Lambda(\n",
        "      lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
        "      output_shape=backend.int_shape(x)[1:],\n",
        "      arguments={'scale': scale},\n",
        "      name=block_name)([x, up])\n",
        "  if activation is not None:\n",
        "    x = layers.Activation(activation, name=block_name + '_ac')(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbMNdHh703m8"
      },
      "outputs": [],
      "source": [
        "shape = (160,160,3)\n",
        "def get_inception(InceptionResNetV2):\n",
        "  model = InceptionResNetV2(include_top=1,\n",
        "                            weights=None,#\"imagenet\",\n",
        "                            input_tensor=None,\n",
        "                            input_shape=shape,\n",
        "                            pooling=None,\n",
        "                            classes=49,\n",
        "                            classifier_activation=\"softmax\")\n",
        "\n",
        "  # add some dropout at the end:\n",
        "  l2 = model.layers[-3]\n",
        "  l1 = model.layers[-2]\n",
        "  output = model.layers[-1]\n",
        "\n",
        "  dropout1 = tf.keras.layers.Dropout(0.7)\n",
        "  dropout2 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  x = dropout1(l2.output)\n",
        "  x = l1(x)\n",
        "  x = dropout2(x)\n",
        "  outputdrop = output(x)\n",
        "\n",
        "  modeldrop = Model(inputs=model.input, outputs=outputdrop)\n",
        "  return modeldrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDJ6rSi0JrsB",
        "outputId": "bb710d27-d786-4a40-cd14-a81e9297b229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " separable_conv2d (SeparableCon  (None, 79, 79, 32)  123         ['input_1[0][0]']                \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 79, 79, 32)  96          ['separable_conv2d[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 79, 79, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (SeparableC  (None, 77, 77, 32)  1312        ['activation[0][0]']             \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 77, 77, 32)  96          ['separable_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 77, 77, 32)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 38, 38, 32)   0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (SeparableC  (None, 38, 38, 80)  2592        ['max_pooling2d[0][0]']          \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 38, 38, 80)  240         ['separable_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 38, 38, 80)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (SeparableC  (None, 36, 36, 192)  16080      ['activation_2[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 36, 36, 192)  576        ['separable_conv2d_3[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 36, 36, 192)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 17, 17, 192)  0          ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (SeparableC  (None, 17, 17, 48)  9408        ['max_pooling2d_1[0][0]']        \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv2d_7 (SeparableC  (None, 17, 17, 64)  12480       ['max_pooling2d_1[0][0]']        \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 17, 17, 48)  144         ['separable_conv2d_5[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 17, 17, 64)  192         ['separable_conv2d_7[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 17, 17, 48)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 17, 17, 192)  0          ['max_pooling2d_1[0][0]']        \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (SeparableC  (None, 17, 17, 96)  18624       ['max_pooling2d_1[0][0]']        \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv2d_6 (SeparableC  (None, 17, 17, 64)  4272        ['activation_5[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv2d_8 (SeparableC  (None, 17, 17, 96)  6720        ['activation_7[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " separable_conv2d_9 (SeparableC  (None, 17, 17, 64)  12480       ['average_pooling2d[0][0]']      \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 17, 17, 96)  288         ['separable_conv2d_4[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 17, 17, 64)  192         ['separable_conv2d_6[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 17, 17, 96)  288         ['separable_conv2d_8[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 17, 17, 64)  192         ['separable_conv2d_9[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 17, 17, 96)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 17, 17, 96)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " mixed_5b (Concatenate)         (None, 17, 17, 320)  0           ['activation_4[0][0]',           \n",
            "                                                                  'activation_6[0][0]',           \n",
            "                                                                  'activation_8[0][0]',           \n",
            "                                                                  'activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " separable_conv2d_11 (Separable  (None, 17, 17, 32)  10560       ['mixed_5b[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_13 (Separable  (None, 17, 17, 32)  10560       ['mixed_5b[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_11[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_13[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_10 (Separable  (None, 17, 17, 32)  10560       ['mixed_5b[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_12 (Separable  (None, 17, 17, 32)  1312        ['activation_11[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_14 (Separable  (None, 17, 17, 64)  2336        ['activation_13[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_10[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_12[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 17, 17, 64)  192         ['separable_conv2d_14[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " block35_1_mixed (Concatenate)  (None, 17, 17, 128)  0           ['activation_10[0][0]',          \n",
            "                                                                  'activation_12[0][0]',          \n",
            "                                                                  'activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " block35_1_conv (SeparableConv2  (None, 17, 17, 320)  41408      ['block35_1_mixed[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block35_1 (Lambda)             (None, 17, 17, 320)  0           ['mixed_5b[0][0]',               \n",
            "                                                                  'block35_1_conv[0][0]']         \n",
            "                                                                                                  \n",
            " block35_1_ac (Activation)      (None, 17, 17, 320)  0           ['block35_1[0][0]']              \n",
            "                                                                                                  \n",
            " separable_conv2d_16 (Separable  (None, 17, 17, 32)  10560       ['block35_1_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_18 (Separable  (None, 17, 17, 32)  10560       ['block35_1_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_16[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_18[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_15 (Separable  (None, 17, 17, 32)  10560       ['block35_1_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_17 (Separable  (None, 17, 17, 32)  1312        ['activation_16[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_19 (Separable  (None, 17, 17, 64)  2336        ['activation_18[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_15[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 17, 17, 32)  96          ['separable_conv2d_17[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 17, 17, 64)  192         ['separable_conv2d_19[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " block35_2_mixed (Concatenate)  (None, 17, 17, 128)  0           ['activation_15[0][0]',          \n",
            "                                                                  'activation_17[0][0]',          \n",
            "                                                                  'activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " block35_2_conv (SeparableConv2  (None, 17, 17, 320)  41408      ['block35_2_mixed[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block35_2 (Lambda)             (None, 17, 17, 320)  0           ['block35_1_ac[0][0]',           \n",
            "                                                                  'block35_2_conv[0][0]']         \n",
            "                                                                                                  \n",
            " block35_2_ac (Activation)      (None, 17, 17, 320)  0           ['block35_2[0][0]']              \n",
            "                                                                                                  \n",
            " separable_conv2d_21 (Separable  (None, 17, 17, 256)  82240      ['block35_2_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 17, 17, 256)  768        ['separable_conv2d_21[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 17, 17, 256)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_20 (Separable  (None, 8, 8, 384)   125760      ['block35_2_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_22 (Separable  (None, 8, 8, 384)   100608      ['activation_21[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 8, 8, 384)   1152        ['separable_conv2d_20[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 8, 8, 384)   1152        ['separable_conv2d_22[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)   0           ['block35_2_ac[0][0]']           \n",
            "                                                                                                  \n",
            " mixed_6a (Concatenate)         (None, 8, 8, 1088)   0           ['activation_20[0][0]',          \n",
            "                                                                  'activation_22[0][0]',          \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " separable_conv2d_24 (Separable  (None, 8, 8, 128)   140352      ['mixed_6a[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 8, 8, 128)   384         ['separable_conv2d_24[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_23 (Separable  (None, 8, 8, 192)   209984      ['mixed_6a[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_25 (Separable  (None, 8, 8, 192)   25472       ['activation_24[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 8, 8, 192)   576         ['separable_conv2d_23[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 8, 8, 192)   576         ['separable_conv2d_25[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " block17_1_mixed (Concatenate)  (None, 8, 8, 384)    0           ['activation_23[0][0]',          \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " block17_1_conv (SeparableConv2  (None, 8, 8, 1088)  419264      ['block17_1_mixed[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block17_1 (Lambda)             (None, 8, 8, 1088)   0           ['mixed_6a[0][0]',               \n",
            "                                                                  'block17_1_conv[0][0]']         \n",
            "                                                                                                  \n",
            " block17_1_ac (Activation)      (None, 8, 8, 1088)   0           ['block17_1[0][0]']              \n",
            "                                                                                                  \n",
            " separable_conv2d_27 (Separable  (None, 8, 8, 128)   140352      ['block17_1_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 128)   384         ['separable_conv2d_27[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_26 (Separable  (None, 8, 8, 192)   209984      ['block17_1_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_28 (Separable  (None, 8, 8, 192)   25472       ['activation_27[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 192)   576         ['separable_conv2d_26[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 192)   576         ['separable_conv2d_28[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " block17_2_mixed (Concatenate)  (None, 8, 8, 384)    0           ['activation_26[0][0]',          \n",
            "                                                                  'activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " block17_2_conv (SeparableConv2  (None, 8, 8, 1088)  419264      ['block17_2_mixed[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block17_2 (Lambda)             (None, 8, 8, 1088)   0           ['block17_1_ac[0][0]',           \n",
            "                                                                  'block17_2_conv[0][0]']         \n",
            "                                                                                                  \n",
            " block17_2_ac (Activation)      (None, 8, 8, 1088)   0           ['block17_2[0][0]']              \n",
            "                                                                                                  \n",
            " separable_conv2d_29 (Separable  (None, 8, 8, 256)   279616      ['block17_2_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_31 (Separable  (None, 8, 8, 256)   279616      ['block17_2_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_33 (Separable  (None, 8, 8, 256)   279616      ['block17_2_ac[0][0]']           \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 256)   768         ['separable_conv2d_29[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 8, 8, 256)   768         ['separable_conv2d_31[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 8, 8, 256)   768         ['separable_conv2d_33[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_30 (Separable  (None, 3, 3, 384)   100608      ['activation_29[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_32 (Separable  (None, 3, 3, 288)   76032       ['activation_31[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_34 (Separable  (None, 3, 3, 320)   84224       ['activation_33[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 3, 3, 384)   1152        ['separable_conv2d_30[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 3, 3, 288)   864         ['separable_conv2d_32[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 3, 3, 320)   960         ['separable_conv2d_34[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 3, 3, 288)    0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 1088)  0           ['block17_2_ac[0][0]']           \n",
            "                                                                                                  \n",
            " mixed_7a (Concatenate)         (None, 3, 3, 2080)   0           ['activation_30[0][0]',          \n",
            "                                                                  'activation_32[0][0]',          \n",
            "                                                                  'activation_34[0][0]',          \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " separable_conv2d_36 (Separable  (None, 3, 3, 192)   401440      ['mixed_7a[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_36[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_35 (Separable  (None, 3, 3, 192)   401440      ['mixed_7a[0][0]']               \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_37 (Separable  (None, 3, 3, 256)   49728       ['activation_36[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_35[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 3, 3, 256)   768         ['separable_conv2d_37[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 3, 3, 256)    0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " block8_1_mixed (Concatenate)   (None, 3, 3, 448)    0           ['activation_35[0][0]',          \n",
            "                                                                  'activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " block8_1_conv (SeparableConv2D  (None, 3, 3, 2080)  934368      ['block8_1_mixed[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block8_1 (Lambda)              (None, 3, 3, 2080)   0           ['mixed_7a[0][0]',               \n",
            "                                                                  'block8_1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " block8_1_ac (Activation)       (None, 3, 3, 2080)   0           ['block8_1[0][0]']               \n",
            "                                                                                                  \n",
            " separable_conv2d_39 (Separable  (None, 3, 3, 192)   401440      ['block8_1_ac[0][0]']            \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_39[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_38 (Separable  (None, 3, 3, 192)   401440      ['block8_1_ac[0][0]']            \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_40 (Separable  (None, 3, 3, 256)   49728       ['activation_39[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_38[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 3, 3, 256)   768         ['separable_conv2d_40[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 3, 3, 256)    0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " block8_2_mixed (Concatenate)   (None, 3, 3, 448)    0           ['activation_38[0][0]',          \n",
            "                                                                  'activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " block8_2_conv (SeparableConv2D  (None, 3, 3, 2080)  934368      ['block8_2_mixed[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block8_2 (Lambda)              (None, 3, 3, 2080)   0           ['block8_1_ac[0][0]',            \n",
            "                                                                  'block8_2_conv[0][0]']          \n",
            "                                                                                                  \n",
            " block8_2_ac (Activation)       (None, 3, 3, 2080)   0           ['block8_2[0][0]']               \n",
            "                                                                                                  \n",
            " separable_conv2d_42 (Separable  (None, 3, 3, 192)   401440      ['block8_2_ac[0][0]']            \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_42[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " separable_conv2d_41 (Separable  (None, 3, 3, 192)   401440      ['block8_2_ac[0][0]']            \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " separable_conv2d_43 (Separable  (None, 3, 3, 256)   49728       ['activation_42[0][0]']          \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 3, 3, 192)   576         ['separable_conv2d_41[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 3, 3, 256)   768         ['separable_conv2d_43[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 3, 3, 256)    0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " block8_10_mixed (Concatenate)  (None, 3, 3, 448)    0           ['activation_41[0][0]',          \n",
            "                                                                  'activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " block8_10_conv (SeparableConv2  (None, 3, 3, 2080)  934368      ['block8_10_mixed[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block8_10 (Lambda)             (None, 3, 3, 2080)   0           ['block8_2_ac[0][0]',            \n",
            "                                                                  'block8_10_conv[0][0]']         \n",
            "                                                                                                  \n",
            " conv_7b (SeparableConv2D)      (None, 3, 3, 1536)   3196960     ['block8_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv_7b_bn (BatchNormalization  (None, 3, 3, 1536)  4608        ['conv_7b[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv_7b_ac (Activation)        (None, 3, 3, 1536)   0           ['conv_7b_bn[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 3, 3, 1536)   0           ['conv_7b_ac[0][0]']             \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 1536)        0           ['dropout[0][0]']                \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 1536)         0           ['avg_pool[1][0]']               \n",
            "                                                                                                  \n",
            " predictions (Dense)            (None, 49)           75313       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,894,476\n",
            "Trainable params: 11,877,644\n",
            "Non-trainable params: 16,832\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_inception(InceptionResNetV2)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SpB-4zX-EtV"
      },
      "source": [
        "## Compile the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGHppVoRiIL0"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BueYa4qJMR9D"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RprexTkMWAZ"
      },
      "source": [
        "## Data Augmentation (done on the fly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84I5YD83uKlX"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUj7_GokEuwC"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                          horizontal_flip=True,\n",
        "                                                          #vertical_flip=True,\n",
        "                                                          #zoom_range=[0.97,1.03],\n",
        "                                                          )\n",
        "\n",
        "# for categorical:\n",
        "it = datagen.flow(data.trainimg, train_out, batch_size=batch_size)\n",
        "val = (data.valimg, val_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPu-LzF-MYQj"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjBs5EUehgMy",
        "outputId": "0d2c24dd-946b-4c88-d0ef-386a54236a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/28\n",
            "29/29 [==============================] - 47s 702ms/step - loss: 3.3127 - accuracy: 0.1975 - val_loss: 3.8838 - val_accuracy: 0.0379\n",
            "Epoch 2/28\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 2.4343 - accuracy: 0.3793 - val_loss: 3.8835 - val_accuracy: 0.0379\n",
            "Epoch 3/28\n",
            "29/29 [==============================] - 18s 625ms/step - loss: 1.8560 - accuracy: 0.5102 - val_loss: 3.8995 - val_accuracy: 0.0126\n",
            "Epoch 4/28\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 1.4042 - accuracy: 0.6207 - val_loss: 3.9350 - val_accuracy: 0.0126\n",
            "Epoch 5/28\n",
            "29/29 [==============================] - 19s 644ms/step - loss: 1.0826 - accuracy: 0.6989 - val_loss: 3.9959 - val_accuracy: 0.0126\n",
            "Epoch 6/28\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.8280 - accuracy: 0.7646 - val_loss: 4.0959 - val_accuracy: 0.0126\n",
            "Epoch 7/28\n",
            "29/29 [==============================] - 18s 611ms/step - loss: 0.6544 - accuracy: 0.8140 - val_loss: 4.2371 - val_accuracy: 0.0442\n",
            "Epoch 8/28\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.5001 - accuracy: 0.8596 - val_loss: 4.4136 - val_accuracy: 0.0442\n",
            "Epoch 9/28\n",
            "29/29 [==============================] - 18s 616ms/step - loss: 0.3819 - accuracy: 0.8902 - val_loss: 4.6751 - val_accuracy: 0.0379\n",
            "Epoch 10/28\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.3095 - accuracy: 0.9116 - val_loss: 4.9331 - val_accuracy: 0.0379\n",
            "Epoch 11/28\n",
            "29/29 [==============================] - 18s 612ms/step - loss: 0.2381 - accuracy: 0.9347 - val_loss: 5.2280 - val_accuracy: 0.0379\n",
            "Epoch 12/28\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.1942 - accuracy: 0.9477 - val_loss: 5.5838 - val_accuracy: 0.0379\n",
            "Epoch 13/28\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.1703 - accuracy: 0.9526 - val_loss: 5.9223 - val_accuracy: 0.0379\n",
            "Epoch 14/28\n",
            "29/29 [==============================] - 18s 610ms/step - loss: 0.1413 - accuracy: 0.9653 - val_loss: 6.5780 - val_accuracy: 0.0379\n",
            "Epoch 15/28\n",
            "29/29 [==============================] - 18s 613ms/step - loss: 0.1250 - accuracy: 0.9681 - val_loss: 6.6674 - val_accuracy: 0.0379\n",
            "Epoch 16/28\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.0911 - accuracy: 0.9775 - val_loss: 6.3188 - val_accuracy: 0.0379\n",
            "Epoch 17/28\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.0907 - accuracy: 0.9765 - val_loss: 5.7483 - val_accuracy: 0.0505\n",
            "Epoch 18/28\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.0723 - accuracy: 0.9835 - val_loss: 5.1390 - val_accuracy: 0.1021\n",
            "Epoch 19/28\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.0457 - accuracy: 0.9937 - val_loss: 3.3607 - val_accuracy: 0.2516\n",
            "Epoch 20/28\n",
            "29/29 [==============================] - 18s 611ms/step - loss: 0.0434 - accuracy: 0.9912 - val_loss: 2.4333 - val_accuracy: 0.3716\n",
            "Epoch 21/28\n",
            "29/29 [==============================] - 18s 616ms/step - loss: 0.0545 - accuracy: 0.9874 - val_loss: 1.1772 - val_accuracy: 0.6663\n",
            "Epoch 22/28\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.0532 - accuracy: 0.9874 - val_loss: 0.9521 - val_accuracy: 0.7379\n",
            "Epoch 23/28\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.0646 - accuracy: 0.9775 - val_loss: 0.6765 - val_accuracy: 0.8063\n",
            "Epoch 24/28\n",
            "29/29 [==============================] - 18s 611ms/step - loss: 0.0656 - accuracy: 0.9825 - val_loss: 0.7126 - val_accuracy: 0.7937\n",
            "Epoch 25/28\n",
            "29/29 [==============================] - 18s 612ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.4982 - val_accuracy: 0.8600\n",
            "Epoch 26/28\n",
            "29/29 [==============================] - 18s 610ms/step - loss: 0.0440 - accuracy: 0.9895 - val_loss: 0.6436 - val_accuracy: 0.8284\n",
            "Epoch 27/28\n",
            "29/29 [==============================] - 18s 612ms/step - loss: 0.0458 - accuracy: 0.9895 - val_loss: 0.4870 - val_accuracy: 0.8600\n",
            "Epoch 28/28\n",
            "29/29 [==============================] - 18s 617ms/step - loss: 0.0333 - accuracy: 0.9923 - val_loss: 0.4322 - val_accuracy: 0.8832\n"
          ]
        }
      ],
      "source": [
        "redofirststep = True\n",
        "\n",
        "if redofirststep:\n",
        "\n",
        "  K.set_value(model.optimizer.learning_rate, 0.001)\n",
        "\n",
        "  history = model.fit(\n",
        "      it,\n",
        "      validation_data=val,\n",
        "      batch_size=batch_size,\n",
        "      epochs=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKczcTWaiM0F"
      },
      "outputs": [],
      "source": [
        "if redofirststep:\n",
        "  model.save_weights(mydrive / '28epochscrosscat_inceptionresnet_smaller_sepconv.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG6cExwDie3W"
      },
      "source": [
        "Make the network again for triplet training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h4BM_NOieFI"
      },
      "outputs": [],
      "source": [
        "model = get_inception(InceptionResNetV2)\n",
        "loss = tfa.losses.TripletSemiHardLoss(margin=2.0)\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.00005),\n",
        "    loss=loss)\n",
        "\n",
        "# load the previous weights:\n",
        "model.load_weights(mydrive / '28epochscrosscat_inceptionresnet_smaller_sepconv.h5') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE-kbnlKi9Jd"
      },
      "source": [
        "We also need the labels themselves instead of onehots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GWhzL1TiGLe"
      },
      "outputs": [],
      "source": [
        "# data for triplet:\n",
        "it = datagen.flow(data.trainimg, data.trainlabels, batch_size=batch_size)\n",
        "val = (data.valimg, data.vallabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWh4q9YjwDhh",
        "outputId": "283f5d78-dcd9-44d7-ca40-0839a21386ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "29/29 [==============================] - 26s 644ms/step - loss: 0.8305 - val_loss: 1.2737\n",
            "Epoch 2/30\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.7974 - val_loss: 1.2479\n",
            "Epoch 3/30\n",
            "29/29 [==============================] - 18s 608ms/step - loss: 0.7766 - val_loss: 1.2439\n",
            "Epoch 4/30\n",
            "29/29 [==============================] - 18s 611ms/step - loss: 0.7595 - val_loss: 1.2393\n",
            "Epoch 5/30\n",
            "29/29 [==============================] - 18s 610ms/step - loss: 0.7387 - val_loss: 1.2390\n",
            "Epoch 6/30\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.7625 - val_loss: 1.2222\n",
            "Epoch 7/30\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.7267 - val_loss: 1.2086\n",
            "Epoch 8/30\n",
            "29/29 [==============================] - 18s 612ms/step - loss: 0.7281 - val_loss: 1.2014\n",
            "Epoch 9/30\n",
            "29/29 [==============================] - 18s 620ms/step - loss: 0.7369 - val_loss: 1.2070\n",
            "Epoch 10/30\n",
            "29/29 [==============================] - 18s 608ms/step - loss: 0.7381 - val_loss: 1.1966\n",
            "Epoch 11/30\n",
            "29/29 [==============================] - 18s 635ms/step - loss: 0.7200 - val_loss: 1.1953\n",
            "Epoch 12/30\n",
            "29/29 [==============================] - 18s 606ms/step - loss: 0.7150 - val_loss: 1.2096\n",
            "Epoch 13/30\n",
            "29/29 [==============================] - 21s 705ms/step - loss: 0.6974 - val_loss: 1.2036\n",
            "Epoch 14/30\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.6863 - val_loss: 1.1922\n",
            "Epoch 15/30\n",
            "29/29 [==============================] - 18s 613ms/step - loss: 0.7135 - val_loss: 1.2071\n",
            "Epoch 16/30\n",
            "29/29 [==============================] - 18s 609ms/step - loss: 0.7186 - val_loss: 1.2064\n",
            "Epoch 17/30\n",
            "29/29 [==============================] - 18s 612ms/step - loss: 0.7273 - val_loss: 1.2086\n",
            "Epoch 18/30\n",
            "29/29 [==============================] - 19s 641ms/step - loss: 0.6916 - val_loss: 1.1986\n",
            "Epoch 19/30\n",
            "29/29 [==============================] - 18s 618ms/step - loss: 0.7171 - val_loss: 1.1879\n",
            "Epoch 20/30\n",
            "29/29 [==============================] - 18s 613ms/step - loss: 0.7180 - val_loss: 1.1941\n",
            "Epoch 21/30\n",
            "29/29 [==============================] - 18s 616ms/step - loss: 0.6837 - val_loss: 1.1957\n",
            "Epoch 22/30\n",
            "29/29 [==============================] - 18s 613ms/step - loss: 0.6933 - val_loss: 1.2061\n",
            "Epoch 23/30\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.6832 - val_loss: 1.2043\n",
            "Epoch 24/30\n",
            "29/29 [==============================] - 18s 614ms/step - loss: 0.6844 - val_loss: 1.1940\n",
            "Epoch 25/30\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.6883 - val_loss: 1.1955\n",
            "Epoch 26/30\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.6868 - val_loss: 1.1990\n",
            "Epoch 27/30\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 0.6561 - val_loss: 1.1966\n",
            "Epoch 28/30\n",
            "29/29 [==============================] - 18s 619ms/step - loss: 0.6688 - val_loss: 1.1978\n",
            "Epoch 29/30\n",
            "29/29 [==============================] - 18s 621ms/step - loss: 0.6533 - val_loss: 1.1874\n",
            "Epoch 30/30\n",
            "29/29 [==============================] - 18s 616ms/step - loss: 0.6563 - val_loss: 1.1887\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    it,\n",
        "    validation_data=val,\n",
        "    batch_size=batch_size,\n",
        "    epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAyuaTzv1wOt"
      },
      "outputs": [],
      "source": [
        "model.save_weights(mydrive / '28epochscrosscat_30epochstriplet_inceptionresnet_smaller_sepconv.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OX3qMxDMc5s"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvKYDht2Me-e"
      },
      "source": [
        "## Getting the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh_9PpEUwDo5"
      },
      "outputs": [],
      "source": [
        "predtest = model.predict(data.testimg)\n",
        "predtrain = model.predict(data.trainimg)\n",
        "predval = model.predict(data.valimg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ON6fTmMiak"
      },
      "source": [
        "## Just a visualization\n",
        "if embeddings are well made, we should see\n",
        "some kind of clustering here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "0E3WNL4UBJAD",
        "outputId": "fd286a97-4f29-472c-e32c-1a29fe0f0249"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f35602047d0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pred_pca = pca.fit_transform(predtest)\n",
        "labels = data.testlabels\n",
        "\n",
        "plt.scatter(pred_pca[:,0], pred_pca[:,1], c=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJUNr5CMwKK"
      },
      "source": [
        "## Classifying the embeddings to classes using a dense network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fppi7E23AiF0",
        "outputId": "618e039f-d942-4c48-bf8f-2e1452bc3172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 49)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 49)                2450      \n",
            "                                                                 \n",
            " activation_88 (Activation)  (None, 49)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,450\n",
            "Trainable params: 2,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/150\n",
            "1/1 [==============================] - 0s 382ms/step - loss: 3.8941 - val_loss: 3.6981\n",
            "Epoch 2/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 3.6624 - val_loss: 3.4966\n",
            "Epoch 3/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.4400 - val_loss: 3.3045\n",
            "Epoch 4/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 3.2276 - val_loss: 3.1212\n",
            "Epoch 5/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 3.0245 - val_loss: 2.9455\n",
            "Epoch 6/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.8291 - val_loss: 2.7763\n",
            "Epoch 7/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.6403 - val_loss: 2.6126\n",
            "Epoch 8/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 2.4573 - val_loss: 2.4538\n",
            "Epoch 9/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.2796 - val_loss: 2.2997\n",
            "Epoch 10/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.1074 - val_loss: 2.1501\n",
            "Epoch 11/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.9408 - val_loss: 2.0051\n",
            "Epoch 12/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.7800 - val_loss: 1.8647\n",
            "Epoch 13/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.6253 - val_loss: 1.7294\n",
            "Epoch 14/150\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.4773 - val_loss: 1.5997\n",
            "Epoch 15/150\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.3364 - val_loss: 1.4760\n",
            "Epoch 16/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.2033 - val_loss: 1.3590\n",
            "Epoch 17/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.0785 - val_loss: 1.2491\n",
            "Epoch 18/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.9625 - val_loss: 1.1469\n",
            "Epoch 19/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8557 - val_loss: 1.0527\n",
            "Epoch 20/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.7581 - val_loss: 0.9667\n",
            "Epoch 21/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6697 - val_loss: 0.8888\n",
            "Epoch 22/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.5904 - val_loss: 0.8191\n",
            "Epoch 23/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5198 - val_loss: 0.7570\n",
            "Epoch 24/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4574 - val_loss: 0.7023\n",
            "Epoch 25/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4027 - val_loss: 0.6543\n",
            "Epoch 26/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3549 - val_loss: 0.6124\n",
            "Epoch 27/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3134 - val_loss: 0.5761\n",
            "Epoch 28/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2775 - val_loss: 0.5446\n",
            "Epoch 29/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2465 - val_loss: 0.5175\n",
            "Epoch 30/150\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2198 - val_loss: 0.4941\n",
            "Epoch 31/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1967 - val_loss: 0.4739\n",
            "Epoch 32/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1768 - val_loss: 0.4565\n",
            "Epoch 33/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1596 - val_loss: 0.4416\n",
            "Epoch 34/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1447 - val_loss: 0.4286\n",
            "Epoch 35/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1317 - val_loss: 0.4175\n",
            "Epoch 36/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1204 - val_loss: 0.4078\n",
            "Epoch 37/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1106 - val_loss: 0.3994\n",
            "Epoch 38/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1020 - val_loss: 0.3921\n",
            "Epoch 39/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0944 - val_loss: 0.3857\n",
            "Epoch 40/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0877 - val_loss: 0.3801\n",
            "Epoch 41/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0818 - val_loss: 0.3752\n",
            "Epoch 42/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0765 - val_loss: 0.3710\n",
            "Epoch 43/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0718 - val_loss: 0.3672\n",
            "Epoch 44/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0677 - val_loss: 0.3639\n",
            "Epoch 45/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0639 - val_loss: 0.3610\n",
            "Epoch 46/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0606 - val_loss: 0.3585\n",
            "Epoch 47/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0575 - val_loss: 0.3562\n",
            "Epoch 48/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0548 - val_loss: 0.3542\n",
            "Epoch 49/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0523 - val_loss: 0.3524\n",
            "Epoch 50/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0501 - val_loss: 0.3508\n",
            "Epoch 51/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0480 - val_loss: 0.3495\n",
            "Epoch 52/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0462 - val_loss: 0.3482\n",
            "Epoch 53/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0445 - val_loss: 0.3471\n",
            "Epoch 54/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0429 - val_loss: 0.3461\n",
            "Epoch 55/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0414 - val_loss: 0.3452\n",
            "Epoch 56/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0401 - val_loss: 0.3444\n",
            "Epoch 57/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0389 - val_loss: 0.3437\n",
            "Epoch 58/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0377 - val_loss: 0.3431\n",
            "Epoch 59/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0367 - val_loss: 0.3425\n",
            "Epoch 60/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0357 - val_loss: 0.3420\n",
            "Epoch 61/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0348 - val_loss: 0.3415\n",
            "Epoch 62/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0339 - val_loss: 0.3411\n",
            "Epoch 63/150\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0331 - val_loss: 0.3407\n",
            "Epoch 64/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0323 - val_loss: 0.3403\n",
            "Epoch 65/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0316 - val_loss: 0.3400\n",
            "Epoch 66/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0309 - val_loss: 0.3397\n",
            "Epoch 67/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0303 - val_loss: 0.3394\n",
            "Epoch 68/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0297 - val_loss: 0.3391\n",
            "Epoch 69/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0291 - val_loss: 0.3389\n",
            "Epoch 70/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0286 - val_loss: 0.3387\n",
            "Epoch 71/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0280 - val_loss: 0.3384\n",
            "Epoch 72/150\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0275 - val_loss: 0.3383\n",
            "Epoch 73/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0271 - val_loss: 0.3381\n",
            "Epoch 74/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0266 - val_loss: 0.3379\n",
            "Epoch 75/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0262 - val_loss: 0.3377\n",
            "Epoch 76/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0258 - val_loss: 0.3376\n",
            "Epoch 77/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0254 - val_loss: 0.3374\n",
            "Epoch 78/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0250 - val_loss: 0.3373\n",
            "Epoch 79/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0246 - val_loss: 0.3372\n",
            "Epoch 80/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0242 - val_loss: 0.3371\n",
            "Epoch 81/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0239 - val_loss: 0.3369\n",
            "Epoch 82/150\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0236 - val_loss: 0.3368\n",
            "Epoch 83/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0232 - val_loss: 0.3367\n",
            "Epoch 84/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0229 - val_loss: 0.3366\n",
            "Epoch 85/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0226 - val_loss: 0.3366\n",
            "Epoch 86/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0223 - val_loss: 0.3365\n",
            "Epoch 87/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0220 - val_loss: 0.3364\n",
            "Epoch 88/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0217 - val_loss: 0.3363\n",
            "Epoch 89/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0215 - val_loss: 0.3362\n",
            "Epoch 90/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0212 - val_loss: 0.3362\n",
            "Epoch 91/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0209 - val_loss: 0.3361\n",
            "Epoch 92/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0207 - val_loss: 0.3360\n",
            "Epoch 93/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0204 - val_loss: 0.3360\n",
            "Epoch 94/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0202 - val_loss: 0.3359\n",
            "Epoch 95/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0199 - val_loss: 0.3359\n",
            "Epoch 96/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0197 - val_loss: 0.3358\n",
            "Epoch 97/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0195 - val_loss: 0.3358\n",
            "Epoch 98/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0193 - val_loss: 0.3357\n",
            "Epoch 99/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0190 - val_loss: 0.3357\n",
            "Epoch 100/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0188 - val_loss: 0.3356\n",
            "Epoch 101/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0186 - val_loss: 0.3356\n",
            "Epoch 102/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0184 - val_loss: 0.3356\n",
            "Epoch 103/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0182 - val_loss: 0.3355\n",
            "Epoch 104/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0180 - val_loss: 0.3355\n",
            "Epoch 105/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0178 - val_loss: 0.3355\n",
            "Epoch 106/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0176 - val_loss: 0.3355\n",
            "Epoch 107/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0175 - val_loss: 0.3354\n",
            "Epoch 108/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0173 - val_loss: 0.3354\n",
            "Epoch 109/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0171 - val_loss: 0.3354\n",
            "Epoch 110/150\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0169 - val_loss: 0.3354\n",
            "Epoch 111/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0167 - val_loss: 0.3354\n",
            "Epoch 112/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0166 - val_loss: 0.3354\n",
            "Epoch 113/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0164 - val_loss: 0.3354\n",
            "Epoch 114/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0162 - val_loss: 0.3354\n",
            "Epoch 115/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0161 - val_loss: 0.3353\n",
            "Epoch 116/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0159 - val_loss: 0.3353\n",
            "Epoch 117/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0158 - val_loss: 0.3353\n",
            "Epoch 118/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0156 - val_loss: 0.3353\n",
            "Epoch 119/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0155 - val_loss: 0.3353\n",
            "Epoch 120/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0153 - val_loss: 0.3353\n",
            "Epoch 121/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0152 - val_loss: 0.3354\n",
            "Epoch 122/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0150 - val_loss: 0.3354\n",
            "Epoch 123/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0149 - val_loss: 0.3354\n",
            "Epoch 124/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0147 - val_loss: 0.3354\n",
            "Epoch 125/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0146 - val_loss: 0.3354\n",
            "Epoch 126/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0145 - val_loss: 0.3354\n",
            "Epoch 127/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0143 - val_loss: 0.3354\n",
            "Epoch 128/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0142 - val_loss: 0.3354\n",
            "Epoch 129/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0141 - val_loss: 0.3354\n",
            "Epoch 130/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0139 - val_loss: 0.3355\n",
            "Epoch 131/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0138 - val_loss: 0.3355\n",
            "Epoch 132/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0137 - val_loss: 0.3355\n",
            "Epoch 133/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0136 - val_loss: 0.3355\n",
            "Epoch 134/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0135 - val_loss: 0.3355\n",
            "Epoch 135/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0133 - val_loss: 0.3356\n",
            "Epoch 136/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0132 - val_loss: 0.3356\n",
            "Epoch 137/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0131 - val_loss: 0.3356\n",
            "Epoch 138/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0130 - val_loss: 0.3357\n",
            "Epoch 139/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0129 - val_loss: 0.3357\n",
            "Epoch 140/150\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0128 - val_loss: 0.3357\n",
            "Epoch 141/150\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0127 - val_loss: 0.3357\n",
            "Epoch 142/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0125 - val_loss: 0.3358\n",
            "Epoch 143/150\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0124 - val_loss: 0.3358\n",
            "Epoch 144/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0123 - val_loss: 0.3358\n",
            "Epoch 145/150\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0122 - val_loss: 0.3359\n",
            "Epoch 146/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0121 - val_loss: 0.3359\n",
            "Epoch 147/150\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0120 - val_loss: 0.3360\n",
            "Epoch 148/150\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0119 - val_loss: 0.3360\n",
            "Epoch 149/150\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0118 - val_loss: 0.3360\n",
            "Epoch 150/150\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0117 - val_loss: 0.3361\n",
            "0.011739825829863548\n",
            "0.3360687494277954\n"
          ]
        }
      ],
      "source": [
        "def getClassifier():\n",
        "    input_shape = (49)\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    layer2 = layers.Dense(49)(inputs)\n",
        "    outputs = layers.Activation('softmax')(layer2)\n",
        "    # Compile the model\n",
        "\n",
        "    model_classifier = Model(inputs=inputs, outputs=outputs)\n",
        "    model_classifier.compile(\n",
        "        optimizer=Adam(0.1),\n",
        "        loss='CategoricalCrossentropy')\n",
        "    return model_classifier\n",
        "    \n",
        "classifier = getClassifier()\n",
        "classifier.summary()\n",
        "# immediately train:\n",
        "history = classifier.fit(predtrain,train_out, \\\n",
        "    batch_size=3000, \\\n",
        "    epochs=150, \\\n",
        "    validation_data=(predval, val_out), \\\n",
        "    shuffle=True, verbose=1)\n",
        "print(history.history['loss'][-1])\n",
        "print(history.history['val_loss'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0nFfvvyA3hE",
        "outputId": "a922edfa-bb90-4dec-9b78-bae945114a49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9305263157894736"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outtest = classifier(predtest)\n",
        "mypred = np.argmax(outtest, axis=1)\n",
        "np.sum(mypred == data.testlabels) / mypred.size"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[Inception_smaller]_train_image_embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
