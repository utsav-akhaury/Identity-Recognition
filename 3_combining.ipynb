{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdir = Path('embeddings')\n",
    "lbldir = Path('labels')\n",
    "\n",
    "NClass = 49\n",
    "\n",
    "# noisy: to load embeddings of a lesser quality for images (80% accuracy\n",
    "# on their own)\n",
    "NOISY = False\n",
    "\n",
    "# load facenet generated encodings for testing by defining `add = '_ultimate'`\n",
    "add = '' # '_ultimate'\n",
    "# the embeddings of facenet have size 128:\n",
    "embsize = 128 if add else 49\n",
    "# (our embeddings have size 49 because of \n",
    "# the way we trained our image clusterer)\n",
    "\n",
    "# embedding size for the audio, can vary depending on how many\n",
    "# MFC coefficients and other audio features we use :\n",
    "audioembsize = 207\n",
    "\n",
    "if NOISY:\n",
    "    noisy = 'noisy'\n",
    "else:\n",
    "    noisy = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_val_img = np.load(embdir / f'{noisy}embeddings_val_img{add}.npy')\n",
    "embeddings_test_img = np.load(embdir / f'{noisy}embeddings_test_img{add}.npy')\n",
    "embeddings_train_img = np.load(embdir / f'{noisy}embeddings_train_img{add}.npy')\n",
    "\n",
    "# dividing the audio embeddings by a manually tuned value (`norm`) to make \n",
    "# them comparable in intensity to the image embeddings.\n",
    "norm = 25.\n",
    "embeddings_val_audio = np.load(embdir / 'embeddings_val_audio.npy') / norm\n",
    "embeddings_test_audio = np.load(embdir / 'embeddings_test_audio.npy') / norm\n",
    "embeddings_train_audio = np.load(embdir / 'embeddings_train_audio.npy') / norm\n",
    "\n",
    "tr_lbl  = np.load(lbldir / 'tr_lbl.npy')\n",
    "tst_lbl = np.load(lbldir / 'tst_lbl.npy')\n",
    "val_lbl = np.load(lbldir / 'val_lbl.npy')\n",
    "\n",
    "tr_onehot = to_categorical(tr_lbl)\n",
    "tst_onehot = to_categorical(tst_lbl)\n",
    "val_onehot = to_categorical(val_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the combined embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_val   = np.concatenate((embeddings_val_img, \n",
    "                                   embeddings_val_audio), axis=1)\n",
    "\n",
    "embeddings_test  = np.concatenate((embeddings_test_img, \n",
    "                                   embeddings_test_audio), axis=1)\n",
    "                                \n",
    "# (we do it only for val and test, as for the training set we do some \n",
    "# data augmentation, see the generator `generateTrain` below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremented class selector used by the generator below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "def modulo():\n",
    "  global COUNT\n",
    "  while True:\n",
    "    if COUNT == 0:\n",
    "      yield 0\n",
    "    if COUNT >= NClass-1:\n",
    "      COUNT = 0\n",
    "      yield COUNT\n",
    "    COUNT += 1\n",
    "\n",
    "    yield COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "choicemodulo = modulo()\n",
    "meta_batch_size = 2850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrain():\n",
    "    \"\"\"\n",
    "    here we define a generator which\n",
    "    - selects a class \n",
    "    - randomly chooses an image embedding of this class\n",
    "    - combines it with a randomly chosen embedding of an audio of this class. \n",
    "    (repeats it `batch_size` times.)\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    tuple of numpy arrays\n",
    "        tuple[0]: labels \n",
    "        tuple[1]: embeddings \n",
    "    \"\"\"\n",
    "    while True:\n",
    "        labels = []\n",
    "        a = np.zeros((meta_batch_size, audioembsize), dtype=np.float32)\n",
    "        b = np.zeros((meta_batch_size, embsize), dtype=np.float32)\n",
    "        for i in range(meta_batch_size):\n",
    "            cl = next(choicemodulo)\n",
    "            labels.append(cl)\n",
    "            alls = np.where(tr_lbl==cl)\n",
    "            imgchoice = (np.random.choice(alls[0]), )\n",
    "            audchoice = (np.random.choice(alls[0]), )\n",
    "            imgs = embeddings_train_img[imgchoice]\n",
    "            auds = embeddings_train_audio[audchoice]\n",
    "            a[i, :] = auds\n",
    "            b[i, :] = imgs\n",
    "        yield np.array(labels), np.concatenate([b, a], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = (embeddings_val, val_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our combination classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(Dense(audioembsize + embsize, \n",
    "                    input_shape=(audioembsize + embsize,),\n",
    "                    activation = 'relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(NClass, activation = 'sigmoid'))\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 49)                12593     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,385\n",
      "Trainable params: 78,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 13:46:40.313787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier()\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "gen = generateTrain()\n",
    "\n",
    "# Compile the model\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=loss,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 17.5%. val accuracy: 48.0%\n",
      "accuracy: 65.5%. val accuracy: 85.8%\n",
      "accuracy: 90.5%. val accuracy: 90.8%\n",
      "accuracy: 96.2%. val accuracy: 91.8%\n",
      "accuracy: 98.5%. val accuracy: 92.6%\n",
      "accuracy: 98.9%. val accuracy: 92.9%\n",
      "accuracy: 99.5%. val accuracy: 93.6%\n",
      "accuracy: 99.5%. val accuracy: 94.1%\n",
      "accuracy: 99.8%. val accuracy: 94.6%\n",
      "accuracy: 100.0%. val accuracy: 95.1%\n",
      "accuracy: 99.9%. val accuracy: 95.4%\n",
      "accuracy: 100.0%. val accuracy: 95.4%\n",
      "accuracy: 100.0%. val accuracy: 95.4%\n",
      "accuracy: 100.0%. val accuracy: 95.6%\n",
      "accuracy: 100.0%. val accuracy: 95.8%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 96.0%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n",
      "accuracy: 100.0%. val accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "# train in Ncycles cycles, each time generating new combinations\n",
    "# of randomly concatenated image and audio embeddings.\n",
    "\n",
    "Ncycles = 30\n",
    "\n",
    "for j in range(Ncycles):\n",
    "    lbls, tr = next(gen)\n",
    "    lbls = to_categorical(lbls)\n",
    "    train = (tr, lbls)\n",
    "\n",
    "    epochs_per_cycle = 3\n",
    "    \n",
    "    history = classifier.fit(train[0], train[1],\n",
    "                             validation_data=val,\n",
    "                             batch_size=2000,\n",
    "                             epochs=epochs_per_cycle,\n",
    "                             shuffle=True, \n",
    "                             verbose=0)\n",
    "    floss = classifier.history.history['accuracy'][-1]\n",
    "    vloss = classifier.history.history['val_accuracy'][-1]\n",
    "    print(f\"accuracy: {floss*100:.01f}%. val accuracy: {100*vloss:.01f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 95.4%\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(classifier(embeddings_test), axis=1)\n",
    "print(f\"Test accuracy : {np.sum(pred==tst_lbl)/pred.size*100:.01f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
